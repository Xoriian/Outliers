{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpL79uuG7aS-",
        "outputId": "7ad6113a-00f5-43a1-85ff-eaddaa9a25ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.8/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from lime) (1.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lime) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.8/dist-packages (from lime) (1.0.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.8/dist-packages (from lime) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from lime) (4.64.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime) (2.6.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime) (1.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->lime) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->lime) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BDfqbp2VEOG5"
      },
      "outputs": [],
      "source": [
        "import utils_data as utils\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import gc\n",
        "\n",
        "from lime import lime_text\n",
        "from numpy import random\n",
        "from collections import Counter\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AFaNzmXEOG6"
      },
      "source": [
        "### Importation du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5WcGG-cWEOG7"
      },
      "outputs": [],
      "source": [
        "text_train, text_val = utils.get_data_split()\n",
        "label_train, label_val = utils.get_labels_split()\n",
        "text_test = utils.get_test()\n",
        "\n",
        "merged_train = utils.merge_data_labels(text_train, label_train)\n",
        "merged_val = utils.merge_data_labels(text_val, label_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8BIsCE_PJ9tf"
      },
      "outputs": [],
      "source": [
        "# # Solution pour randomiser la séparation train / val\n",
        "text_all = utils.get_data()\n",
        "labels_all = utils.get_labels()\n",
        "limit = int(len(labels_all) * 0.8) + 1\n",
        "\n",
        "merged_all = utils.merge_data_labels(text_all, labels_all)\n",
        "random.shuffle(merged_all)\n",
        "\n",
        "merged_train, merged_val = merged_all[:limit], merged_all[limit:] # Séparation entre les données de train et de validation (80% - 20%)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter(labels_all[:, 1].tolist())\n",
        "label_train = np.asarray([list(e) for e in merged_train])[:, 1].astype(int)\n",
        "\n",
        "class_weights = [len(label_train) / counter[i] for i in range(len(counter))]\n",
        "weights = [class_weights[label_train[i]] for i in range(len(label_train))]\n",
        "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), len(label_train))"
      ],
      "metadata": {
        "id": "qBZjAw0AF1ql"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GZJfwPWEOG7"
      },
      "source": [
        "Note importante : toute la suite est inspiré de : https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks, ainsi que du cours IFT 6135-A2022."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty6n7mr-EOG7"
      },
      "source": [
        "### Création de tokens à partir des phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H9b0_HSEOG7",
        "outputId": "c31db873-c831-4501-c515-5eb253b5d8ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "781347"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([text_train, text_val, text_test]), min_freq=1, specials=[\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857_mKwVEOG8"
      },
      "source": [
        "### Préparation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BKZvzdKSEOG8"
      },
      "outputs": [],
      "source": [
        "data_train, data_val = to_map_style_dataset(merged_train), to_map_style_dataset(merged_val)\n",
        "data_test = to_map_style_dataset(text_test)\n",
        "\n",
        "target_classes = [\"negative\", \"neutral\", \"positive\"]\n",
        "max_words = 25\n",
        "\n",
        "def vectorize_batch(batch):\n",
        "    X, Y = list(zip(*batch))\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y, dtype = torch.long)\n",
        "\n",
        "\n",
        "def vectorize_test_batch(batch):\n",
        "    X = list(batch)\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.int32)\n",
        "\n",
        "train_loader = DataLoader(data_train, batch_size=1024, collate_fn=vectorize_batch, sampler = sampler)\n",
        "val_loader = DataLoader(data_val, batch_size=1024, collate_fn=vectorize_batch)\n",
        "test_loader = DataLoader(data_test, batch_size=1024, collate_fn=vectorize_test_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HDOXv9TEOG8"
      },
      "source": [
        "### Définition du RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qOy7TJHbEOG9"
      },
      "outputs": [],
      "source": [
        "embed_len = 50\n",
        "hidden_dim = 50\n",
        "n_layers = 4\n",
        "p_dropout = 0.5\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "        self.rnn = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch).to(device)\n",
        "        output, hidden = self.rnn(self.dropout(embeddings))\n",
        "        return F.softmax(self.linear(output[:,-1]), dim = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC8-YNWcEOG9"
      },
      "source": [
        "### Entraînement du RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EnpWFX1qEOG9"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_true, Y_preds, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            Y_true.append(Y)\n",
        "            #Y = torch.tensor(utils.to_one_hot(Y.numpy(), len(target_classes)))\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_true = torch.cat(Y_true)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "        print(\"Valid Loss : {:.3f} | Valid Acc : {:.3f}\".format(torch.tensor(losses).mean(), accuracy_score(Y_true.detach().cpu().numpy(), Y_preds.detach().cpu().numpy())))\n",
        "\n",
        "\n",
        "def train(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for i in range(1, epochs+1):\n",
        "        Y_true, Y_preds, losses = [],[],[]\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_true.append(Y)\n",
        "            #Y = torch.tensor(utils.to_one_hot(Y.numpy(), len(target_classes)))\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        Y_true = torch.cat(Y_true)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "            \n",
        "        print(\"Epoch {} | Train Loss : {:.3f} | Train acc : {:.3f}\".format(i, torch.tensor(losses).mean(), accuracy_score(Y_true.detach().cpu().numpy(), Y_preds.detach().cpu().numpy())))\n",
        "        evaluate(model, loss_fn, val_loader)\n",
        "        model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "KflHzO4sEOG-",
        "outputId": "1c7c8236-954f-46f8-8d1d-90da652d8621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 29/813 [00:35<16:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-00fef3e42dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-81c9884fec33>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mY_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mY_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    158\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    214\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "rnn_classifier = RNNClassifier().to(device)\n",
        "optimizer = Adam(rnn_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "train(rnn_classifier, loss_fn, optimizer, train_loader, val_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM9hYKfHEOG-"
      },
      "source": [
        "### Prédictions sur l'ensemble de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swTDpMYdEOG-"
      },
      "outputs": [],
      "source": [
        "def predict(model, loader):\n",
        "    Y_preds = []\n",
        "    for X in tqdm(loader):\n",
        "        X = X.to(device)\n",
        "        preds = model(X)\n",
        "        Y_preds.append(preds.detach().cpu())\n",
        "    gc.collect()\n",
        "    Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "    return Y_preds.argmax(dim=-1).numpy()\n",
        "\n",
        "Y_preds = predict(rnn_classifier, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLRzU1rvEOG-"
      },
      "source": [
        "### Enregistrement des résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGTsXRzuEOG_"
      },
      "outputs": [],
      "source": [
        "utils.save_results(Y_preds, \"RNN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSKKH6Pf5nA_"
      },
      "source": [
        "### Explicabilité : Matrice de confusion et LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxwumCxzhwcE"
      },
      "outputs": [],
      "source": [
        "val_loader_test = DataLoader(text_val, batch_size=1024, collate_fn=vectorize_test_batch)\n",
        "Y_preds_val = predict(rnn_classifier, val_loader_test)\n",
        "Y_actual_val = label_val[:, 1]\n",
        "\n",
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual_val, Y_preds_val)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual_val, Y_preds_val, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual_val, Y_preds_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eftm4iB5snH"
      },
      "outputs": [],
      "source": [
        "X_test_text, Y_test = [], []\n",
        "for X, Y in merged_val:\n",
        "    X_test_text.append(X)\n",
        "    Y_test.append(Y)\n",
        "\n",
        "explainer = lime_text.LimeTextExplainer(class_names=target_classes, verbose=True)\n",
        "\n",
        "def make_predictions_lime(X_batch_text):\n",
        "    X = [vocab(tokenizer(text)) for text in X_batch_text]\n",
        "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "    preds = rnn_classifier(torch.tensor(X, dtype=torch.int32, device = device))\n",
        "    return preds.detach().cpu().numpy()\n",
        "\n",
        "idx = int(random.uniform(0, len(Y_test), 1))\n",
        "X = [vocab(tokenizer(text)) for text in X_test_text[idx:idx+1]]\n",
        "X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "preds = rnn_classifier(torch.tensor(X, dtype=torch.int32, device = device))\n",
        "\n",
        "explanation = explainer.explain_instance(X_test_text[idx], classifier_fn=make_predictions_lime,\n",
        "                                         labels=Y_test[idx:idx+1])\n",
        "explanation.show_in_notebook()\n",
        "print(\"Prediction : \", target_classes[preds.argmax()])\n",
        "print(\"Actual :     \", target_classes[Y_test[idx]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c5fb5e78068286ad4ec2111baba118e0d35af9e9b40524b1c6e140f245c1a137"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}