{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDfqbp2VEOG5",
        "outputId": "56350d4e-61c2-4b6e-9999-8b0e002ce828"
      },
      "outputs": [],
      "source": [
        "import utils_data as utils\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torch.optim import Adam\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import gc\n",
        "import math\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AFaNzmXEOG6"
      },
      "source": [
        "### Importation du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WcGG-cWEOG7"
      },
      "outputs": [],
      "source": [
        "### Décommentez pour avoir des datasets identiques à chaque test ###\n",
        "\n",
        "# # Récupération du contenu des différents datasets\n",
        "# text_train, text_val = utils.get_data_split()\n",
        "# label_train, label_val = utils.get_labels_split()\n",
        "# text_test = utils.get_test()\n",
        "\n",
        "# # Fusion des textes et des labels sous forme d'une liste de tuple\n",
        "# merged_train = utils.merge_data_labels(text_train, label_train)\n",
        "# merged_val = utils.merge_data_labels(text_val, label_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BIsCE_PJ9tf"
      },
      "outputs": [],
      "source": [
        "### Commentez pour avoir des datasets identiques à chaque test ###\n",
        "\n",
        "# Solution pour randomiser la séparation train / val\n",
        "text_all = utils.get_data()\n",
        "labels_all = utils.get_labels()\n",
        "limit = int(len(labels_all) * 0.8) + 1\n",
        "\n",
        "merged_all = utils.merge_data_labels(text_all, labels_all)\n",
        "np.random.shuffle(merged_all)\n",
        "\n",
        "merged_train, merged_val = merged_all[:limit], merged_all[limit:] # Séparation entre les données de train et de validation (80% - 20%)\n",
        "\n",
        "text_test = utils.get_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GZJfwPWEOG7"
      },
      "source": [
        "Note importante : toute la suite est inspiré de : https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks, ainsi que https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/ et du cours IFT 6135-A2022."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty6n7mr-EOG7"
      },
      "source": [
        "### Création de tokens à partir des phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H9b0_HSEOG7"
      },
      "outputs": [],
      "source": [
        "# On construit notre vocabulaire à partir des mots de la base de données\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([text_all]), min_freq=1, specials=[\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOpeD6M8L8gM"
      },
      "source": [
        "### Hyperparamètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMAYrY3zL8gM"
      },
      "outputs": [],
      "source": [
        "target_classes = [\"negative\", \"neutral\", \"positive\"]\n",
        "max_words = 50\n",
        "embed_len = 50\n",
        "n_heads = 5\n",
        "n_layers = 3\n",
        "p_dropout = 0.1\n",
        "epochs = 5\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857_mKwVEOG8"
      },
      "source": [
        "### Préparation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKZvzdKSEOG8"
      },
      "outputs": [],
      "source": [
        "# Comptage du nombre d'exemples par classe pour établir des poids et un \"sampler\" (sur l'ensemble des données d'entraînement choisies)\n",
        "label_train = np.asarray([list(e) for e in merged_train])[:, 1].astype(int)\n",
        "counter = Counter(label_train.tolist())\n",
        "\n",
        "class_weights = [len(label_train) / counter[i] for i in range(len(counter))]\n",
        "weights = [class_weights[label_train[i]] for i in range(len(label_train))]\n",
        "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), len(label_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNvLyTUaL8gN"
      },
      "outputs": [],
      "source": [
        "# Préparation des données sous un format lisible par PyTorch\n",
        "data_train, data_val = to_map_style_dataset(merged_train), to_map_style_dataset(merged_val)\n",
        "data_test = to_map_style_dataset(text_test)\n",
        "\n",
        "# Fonction servant à transformer nos données (textes) sous formes de nombres.\n",
        "def vectorize_batch(batch):\n",
        "    X, Y = list(zip(*batch))\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y, dtype = torch.long)\n",
        "\n",
        "# Fonction servant à transformer nos données (textes) sous formes de nombres (dans le cas de données de test, on a pas de labels)\n",
        "def vectorize_test_batch(batch):\n",
        "    X = list(batch)\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.int32)\n",
        "\n",
        "# Préparation des DataLoader pouvant être lu dans les modèles PyTorch\n",
        "train_loader = DataLoader(data_train, batch_size=512, collate_fn=vectorize_batch, sampler=sampler)\n",
        "val_loader = DataLoader(data_val, batch_size=512, collate_fn=vectorize_batch)\n",
        "test_loader = DataLoader(data_test, batch_size=512, collate_fn=vectorize_test_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HDOXv9TEOG8"
      },
      "source": [
        "### Définition du Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF3FEJHoMO_l"
      },
      "outputs": [],
      "source": [
        "# Classe permettant d'encoder le postionnement des mots dans le texte (pour que le Transformer en tienne compte)\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(vocab_size, d_model)\n",
        "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float()\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOy7TJHbEOG9"
      },
      "outputs": [],
      "source": [
        "# Architecture de notre Transformer\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, nhead=n_heads, dim_feedforward=2048, num_layers=n_layers, dropout=p_dropout, activation=\"relu\", classifier_dropout=p_dropout):\n",
        "\n",
        "    super().__init__()\n",
        "    self.d_model = embed_len\n",
        "\n",
        "    assert self.d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
        "    self.emb = nn.Embedding(num_embeddings=len(vocab), embedding_dim=self.d_model)\n",
        "    self.pos_encoder = PositionalEncoding(d_model=self.d_model, dropout=dropout, vocab_size=len(vocab))\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "    self.classifier = nn.Linear(self.d_model, len(target_classes))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.emb(x) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoder(x)\n",
        "    x = self.transformer_encoder(x)\n",
        "    x = x.mean(dim=1)\n",
        "    x = F.softmax(self.classifier(x), dim=-1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC8-YNWcEOG9"
      },
      "source": [
        "### Entraînement du Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnpWFX1qEOG9"
      },
      "outputs": [],
      "source": [
        "# Fonction d'évaluation du RNN\n",
        "def evaluate(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_true, Y_preds, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            Y_true.append(Y)\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_true = torch.cat(Y_true)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "        print(\"Valid Loss : {:.3f} | Valid Acc : {:.3f}\".format(torch.tensor(losses).mean(), accuracy_score(Y_true.detach().cpu().numpy(), Y_preds.detach().cpu().numpy())))\n",
        "\n",
        "# Fonction d'entraînement du Transformer\n",
        "def train(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for i in range(1, epochs+1):\n",
        "        Y_true, Y_preds, losses = [],[],[]\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_true.append(Y)\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        Y_true = torch.cat(Y_true)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "            \n",
        "        print(\"Epoch {} | Train Loss : {:.3f} | Train acc : {:.3f}\".format(i, torch.tensor(losses).mean(), accuracy_score(Y_true.detach().cpu().numpy(), Y_preds.detach().cpu().numpy())))\n",
        "        evaluate(model, loss_fn, val_loader)\n",
        "        model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KflHzO4sEOG-",
        "outputId": "a7838161-44c0-4571-e898-267245036456"
      },
      "outputs": [],
      "source": [
        "# On déclare le modèle, la fonction de coût et l'optimiseur avant de lancer l'entraînement\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "transformer = Transformer().to(device)\n",
        "optimizer = Adam(transformer.parameters(), lr=learning_rate)\n",
        "\n",
        "train(transformer, loss_fn, optimizer, train_loader, val_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM9hYKfHEOG-"
      },
      "source": [
        "### Prédictions sur l'ensemble de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swTDpMYdEOG-",
        "outputId": "b95e8616-1a82-4b59-e553-eb10e9234cfc"
      },
      "outputs": [],
      "source": [
        "# Fonction de prédiction sur un ensemble de test\n",
        "def predict(model, loader):\n",
        "    Y_preds = []\n",
        "    for X in tqdm(loader):\n",
        "        X = X.to(device)\n",
        "        preds = model(X)\n",
        "        Y_preds.append(preds.detach().cpu())\n",
        "    gc.collect()\n",
        "    Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "    return Y_preds.argmax(dim=-1).numpy()\n",
        "\n",
        "Y_preds = predict(transformer, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLRzU1rvEOG-"
      },
      "source": [
        "### Enregistrement des résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGTsXRzuEOG_"
      },
      "outputs": [],
      "source": [
        "utils.save_results(Y_preds, \"Transformer\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c5fb5e78068286ad4ec2111baba118e0d35af9e9b40524b1c6e140f245c1a137"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
