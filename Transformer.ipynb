{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"zpL79uuG7aS-","executionInfo":{"status":"ok","timestamp":1670276876207,"user_tz":300,"elapsed":632,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["#%pip install lime"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"BDfqbp2VEOG5","executionInfo":{"status":"ok","timestamp":1670277163283,"user_tz":300,"elapsed":329,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["import utils_data as utils\n","import torch\n","import torch.nn as nn \n","import torch.nn.functional as F\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.data import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.functional import to_map_style_dataset\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam\n","\n","import math\n","\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import gc\n","\n","#from lime import lime_text\n","from numpy import random\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"1AFaNzmXEOG6"},"source":["### Importation du dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"5WcGG-cWEOG7","executionInfo":{"status":"ok","timestamp":1670276889284,"user_tz":300,"elapsed":13079,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["text_train, text_val = utils.get_data_split()\n","label_train, label_val = utils.get_labels_split()\n","text_test = utils.get_test()\n","\n","merged_train = utils.merge_data_labels(text_train, label_train)\n","merged_val = utils.merge_data_labels(text_val, label_val)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BIsCE_PJ9tf","executionInfo":{"status":"ok","timestamp":1670276896421,"user_tz":300,"elapsed":7148,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}},"outputId":"2b7697e0-e9ad-4361-b426-694ea014d066"},"outputs":[{"output_type":"stream","name":"stdout","text":["('\"hello tomorrow, my stomach hurts \"\\n', 0)\n"]}],"source":["# # Solution pour randomiser la séparation train / val\n","text_all = utils.get_data()\n","labels_all = utils.get_labels()\n","limit = int(len(labels_all) * 0.8) + 1\n","\n","merged_all = utils.merge_data_labels(text_all, labels_all)\n","random.shuffle(merged_all)\n","\n","merged_train, merged_val = merged_all[:limit], merged_all[limit:] # Séparation entre les données de train et de validation (80% - 20%)\n","print(merged_train[0])"]},{"cell_type":"markdown","metadata":{"id":"4GZJfwPWEOG7"},"source":["Note importante : toute la suite est inspiré de : https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks, ainsi que https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/ et du cours IFT 6135-A2022."]},{"cell_type":"markdown","metadata":{"id":"Ty6n7mr-EOG7"},"source":["### Création de tokens à partir des phrases"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-H9b0_HSEOG7","outputId":"cb992ce0-f685-492d-ae22-41662b704054","executionInfo":{"status":"ok","timestamp":1670276917619,"user_tz":300,"elapsed":21207,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["781347"]},"metadata":{},"execution_count":10}],"source":["tokenizer = get_tokenizer(\"basic_english\")\n","\n","def build_vocabulary(datasets):\n","    for dataset in datasets:\n","        for text in dataset:\n","            yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(build_vocabulary([text_train, text_val, text_test]), min_freq=1, specials=[\"<UNK>\"])\n","vocab.set_default_index(vocab[\"<UNK>\"])\n","len(vocab)"]},{"cell_type":"markdown","metadata":{"id":"857_mKwVEOG8"},"source":["### Préparation des données"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"BKZvzdKSEOG8","executionInfo":{"status":"ok","timestamp":1670276918132,"user_tz":300,"elapsed":523,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["data_train, data_val = to_map_style_dataset(merged_train), to_map_style_dataset(merged_val)\n","data_test = to_map_style_dataset(text_test)\n","\n","target_classes = [\"negative\", \"neutral\", \"positive\"]\n","max_words = 25\n","\n","def vectorize_batch(batch):\n","    X, Y = list(zip(*batch))\n","    X = [vocab(tokenizer(text)) for text in X]\n","    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n","\n","    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y, dtype = torch.long)\n","\n","\n","def vectorize_test_batch(batch):\n","    X = list(batch)\n","    X = [vocab(tokenizer(text)) for text in X]\n","    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n","\n","    return torch.tensor(X, dtype=torch.int32)\n","\n","train_loader = DataLoader(data_train, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\n","val_loader = DataLoader(data_val, batch_size=1024, collate_fn=vectorize_batch)\n","test_loader = DataLoader(data_test, batch_size=1024, collate_fn=vectorize_test_batch)"]},{"cell_type":"markdown","metadata":{"id":"_HDOXv9TEOG8"},"source":["### Définition du Transformer"]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","    \"\"\"\n","\n","    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(vocab_size, d_model)\n","        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(\n","            torch.arange(0, d_model, 2).float()\n","            * (-math.log(10000.0) / d_model)\n","        )\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer(\"pe\", pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, : x.size(1), :]\n","        return self.dropout(x)"],"metadata":{"id":"kF3FEJHoMO_l","executionInfo":{"status":"ok","timestamp":1670276918132,"user_tz":300,"elapsed":8,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"id":"qOy7TJHbEOG9","executionInfo":{"status":"ok","timestamp":1670277275560,"user_tz":300,"elapsed":306,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["embed_len = 50\n","hidden_dim = 50\n","n_layers = 4\n","p_dropout = 0.5\n","\n","class Transformer(nn.Module):\n","  def __init__(self, nhead=5, dim_feedforward=2048, num_layers=6, dropout=0.1, activation=\"relu\", classifier_dropout=0.1):\n","\n","    super().__init__()\n","    self.d_model = embed_len\n","\n","    assert self.d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n","    self.emb = nn.Embedding(num_embeddings=len(vocab), embedding_dim=self.d_model)\n","    self.pos_encoder = PositionalEncoding(d_model=self.d_model, dropout=dropout, vocab_size=len(vocab))\n","    encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n","    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","    self.classifier = nn.Linear(self.d_model, len(target_classes))\n","    \n","\n","  def forward(self, x):\n","    x = self.emb(x) * math.sqrt(self.d_model)\n","    x = self.pos_encoder(x)\n","    x = self.transformer_encoder(x)\n","    x = x.mean(dim=1)\n","    x = self.classifier(x)\n","\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"LC8-YNWcEOG9"},"source":["### Entraînement du Transformer"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"EnpWFX1qEOG9","executionInfo":{"status":"ok","timestamp":1670277134998,"user_tz":300,"elapsed":317,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["def evaluate(model, loss_fn, val_loader):\n","    model.eval()\n","    with torch.no_grad():\n","        Y_true, Y_preds, losses = [],[],[]\n","        for X, Y in val_loader:\n","            Y_true.append(Y)\n","            Y = torch.tensor(utils.to_one_hot(Y.numpy(), len(target_classes)))\n","            X, Y = X.to(device), Y.to(device)\n","            preds = model(X)\n","            loss = loss_fn(preds, Y)\n","            losses.append(loss.item())\n","            Y_preds.append(preds.argmax(dim=-1))\n","\n","        Y_true = torch.cat(Y_true)\n","        Y_preds = torch.cat(Y_preds)\n","\n","        print(\"Valid Loss : {:.3f} | Valid Acc : {:.3f}\".format(torch.tensor(losses).mean(), accuracy_score(Y_true.detach().cpu().numpy(), Y_preds.detach().cpu().numpy())))\n","\n","\n","def train(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n","    for i in range(1, epochs+1):\n","        Y_true, Y_preds, losses = [],[],[]\n","        for X, Y in tqdm(train_loader):\n","            Y_true.append(Y)\n","            Y = torch.tensor(utils.to_one_hot(Y.numpy(), len(target_classes)))\n","            X, Y = X.to(device), Y.to(device)\n","            optimizer.zero_grad()\n","            preds = model(X)\n","            loss = loss_fn(preds, Y)\n","            losses.append(loss.item())\n","            Y_preds.append(preds.argmax(dim=-1))\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","            optimizer.step()\n","\n","        Y_true = torch.cat(Y_true)\n","        Y_preds = torch.cat(Y_preds)\n","            \n","        print(\"Epoch {} | Train Loss : {:.3f} | Train acc : {:.3f}\".format(i, torch.tensor(losses).mean(), accuracy_score(Y_true.detach().cpu().numpy(), Y_preds.detach().cpu().numpy())))\n","        evaluate(model, loss_fn, val_loader)\n","        model.train()"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KflHzO4sEOG-","outputId":"55f2a455-5dbe-4874-d58f-c595428c354f","executionInfo":{"status":"ok","timestamp":1670279688696,"user_tz":300,"elapsed":17410,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 813/813 [04:00<00:00,  3.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1 | Train Loss : 0.573 | Train acc : 0.705\n","Valid Loss : 0.522 | Valid Acc : 0.756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 813/813 [04:00<00:00,  3.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 2 | Train Loss : 0.500 | Train acc : 0.767\n","Valid Loss : 0.491 | Valid Acc : 0.774\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 813/813 [04:00<00:00,  3.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 3 | Train Loss : 0.474 | Train acc : 0.785\n","Valid Loss : 0.483 | Valid Acc : 0.779\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 813/813 [04:00<00:00,  3.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 4 | Train Loss : 0.453 | Train acc : 0.797\n","Valid Loss : 0.481 | Valid Acc : 0.780\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 813/813 [04:00<00:00,  3.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Train Loss : 0.432 | Train acc : 0.810\n","Valid Loss : 0.482 | Valid Acc : 0.781\n"]}],"source":["epochs = 5\n","learning_rate = 0.001\n","\n","loss_fn = nn.CrossEntropyLoss()\n","transformer = Transformer(nhead=5, dim_feedforward=50, dropout=0.0, classifier_dropout=0.0).to(device)\n","optimizer = Adam(transformer.parameters(), lr=learning_rate)\n","\n","train(transformer, loss_fn, optimizer, train_loader, val_loader, epochs)"]},{"cell_type":"markdown","metadata":{"id":"uM9hYKfHEOG-"},"source":["### Prédictions sur l'ensemble de test"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swTDpMYdEOG-","outputId":"5ea28aca-6b36-47c9-b18f-125200e6a52e","executionInfo":{"status":"ok","timestamp":1670279789203,"user_tz":300,"elapsed":62258,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 548/548 [01:02<00:00,  8.83it/s]\n"]}],"source":["def predict(model, loader):\n","    Y_preds = []\n","    for X in tqdm(loader):\n","        X = X.to(device)\n","        preds = model(X)\n","        Y_preds.append(preds.detach().cpu())\n","    gc.collect()\n","    Y_preds = torch.cat(Y_preds)\n","\n","    return F.softmax(Y_preds, dim=-1).argmax(dim=-1).numpy()\n","\n","Y_preds = predict(transformer, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"KLRzU1rvEOG-"},"source":["### Enregistrement des résultats"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"aGTsXRzuEOG_","executionInfo":{"status":"ok","timestamp":1670279790536,"user_tz":300,"elapsed":1353,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["utils.save_results(Y_preds, \"Transformer\")"]},{"cell_type":"markdown","metadata":{"id":"JSKKH6Pf5nA_"},"source":["### Explicabilité : Matrice de confusion et LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxwumCxzhwcE","executionInfo":{"status":"aborted","timestamp":1670276918134,"user_tz":300,"elapsed":7,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["# val_loader_test = DataLoader(text_val, batch_size=1024, collate_fn=vectorize_test_batch)\n","# Y_preds_val = predict(transformer, val_loader_test)\n","# Y_actual_val = label_val[:, 1]\n","\n","# print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual_val, Y_preds_val)))\n","# print(\"\\nClassification Report : \")\n","# print(classification_report(Y_actual_val, Y_preds_val, target_names=target_classes))\n","# print(\"\\nConfusion Matrix : \")\n","# print(confusion_matrix(Y_actual_val, Y_preds_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eftm4iB5snH","executionInfo":{"status":"aborted","timestamp":1670276918134,"user_tz":300,"elapsed":7,"user":{"displayName":"Alexis Raffier","userId":"14866787106580720615"}}},"outputs":[],"source":["# X_test_text, Y_test = [], []\n","# for X, Y in merged_val:\n","#     X_test_text.append(X)\n","#     Y_test.append(Y)\n","\n","# explainer = lime_text.LimeTextExplainer(class_names=target_classes, verbose=True)\n","\n","# def make_predictions_lime(X_batch_text):\n","#     X = [vocab(tokenizer(text)) for text in X_batch_text]\n","#     X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n","#     logits = transformer(torch.tensor(X, dtype=torch.int32, device = device))\n","#     preds = F.softmax(logits, dim=-1)\n","#     return preds.detach().cpu().numpy()\n","\n","# idx = int(random.uniform(0, len(Y_test), 1))\n","# X = [vocab(tokenizer(text)) for text in X_test_text[idx:idx+1]]\n","# X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n","# preds = transformer(torch.tensor(X, dtype=torch.int32, device = device))\n","# preds = F.softmax(preds, dim=-1)\n","\n","# explanation = explainer.explain_instance(X_test_text[idx], classifier_fn=make_predictions_lime,\n","#                                          labels=Y_test[idx:idx+1])\n","# explanation.show_in_notebook()\n","# print(\"Prediction : \", target_classes[preds.argmax()])\n","# print(\"Actual :     \", target_classes[Y_test[idx]])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.8 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"c5fb5e78068286ad4ec2111baba118e0d35af9e9b40524b1c6e140f245c1a137"}}},"nbformat":4,"nbformat_minor":0}